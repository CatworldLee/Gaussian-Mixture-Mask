# Gaussian-Mixture-Mask Attention
The success of Transformers in the field of images has led to the development of many excellent vision transformer variants, but these variants often lack scalability and cannot be used together in many cases. We propose an attention mechanism based on Gaussian Mixture Mask (GMM), which can be applied to any visual transformer, plug and play. On small datasets, compared to the standard vision transformer, the performance is significantly improved without adding almost any parameters. We also compare with other excellent variants such as hierarchical Swin and deep CaiT. Under the same number of parameters, it is also better than these variants. Moreover, when it is inserted into other variants, it can continue to improve the performance of other variants.
